{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "pG2zq5FfW0ap",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0dd1fb4-674a-40ea-ca8f-8e77a304ccd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install -q kaggle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "S6g13-neW4Z-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db2a50db-4f42-42e7-c58e-db466fe12ecd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat '/content/kaggle.json': No such file or directory\n",
            "chmod: cannot access '/root/.kaggle/kaggle.json': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle datasets download kongaevans/speaker-recognition-dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmrWjBLDW7OR",
        "outputId": "e2b7ab46-839f-4d10-9c07-2524ae0def23"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/kongaevans/speaker-recognition-dataset\n",
            "License(s): unknown\n",
            "Downloading speaker-recognition-dataset.zip to /content\n",
            " 97% 223M/231M [00:01<00:00, 167MB/s]\n",
            "100% 231M/231M [00:01<00:00, 141MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip /content/speaker-recognition-dataset.zip"
      ],
      "metadata": {
        "id": "gghYvfJBXF7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "import time\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.metrics import confusion_matrix, classification_report"
      ],
      "metadata": {
        "id": "TnO0DSuI6iYF"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(data_dir):\n",
        "    data = []\n",
        "    labels = []\n",
        "    speakers = ['Benjamin_Netanyau', 'Jens_Stoltenberg', 'Julia_Gillard', 'Magaret_Tarcher', 'Nelson_Mandela']\n",
        "\n",
        "    for speaker in speakers:\n",
        "        speaker_dir = os.path.join(data_dir, speaker)\n",
        "        for filename in os.listdir(speaker_dir):\n",
        "            if filename.endswith('.wav'):\n",
        "                file_path = os.path.join(speaker_dir, filename)\n",
        "                data.append(file_path)\n",
        "                labels.append(speaker)\n",
        "\n",
        "    return data, labels"
      ],
      "metadata": {
        "id": "1epunB7Rrcm_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content/16000_pcm_speeches/'\n",
        "data, labels = load_data(data_dir)"
      ],
      "metadata": {
        "id": "EMxUnLFF6l5d"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(file_path):\n",
        "    y, sr = librosa.load(file_path, sr=16000)\n",
        "    n_mfcc = 13\n",
        "    mfcc = librosa.feature.mfcc(y=y, n_mfcc= n_mfcc)\n",
        "    mfcc_mean = np.mean(mfcc, axis=1)\n",
        "\n",
        "    return mfcc_mean"
      ],
      "metadata": {
        "id": "RCK8SSk16rXq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_features(features):\n",
        "    scaler = StandardScaler()\n",
        "    return scaler.fit_transform(features)"
      ],
      "metadata": {
        "id": "iVNMuodY7mHU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "features = [extract_features(file) for file in data]\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "print(f\"time taken: {execution_time} s\")\n"
      ],
      "metadata": {
        "id": "dg-Sc33c7PBz",
        "outputId": "807291a5-dfff-4d62-8563-a8300c6d8789",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time taken: 65.98510026931763 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Timing feature extraction in parallel\n",
        "start_time_parallel = time.time()\n",
        "\n",
        "with ThreadPoolExecutor() as executor:\n",
        "    features_parallel = executor.map(extract_features, data)\n",
        "\n",
        "\n",
        "end_time_parallel = time.time()\n",
        "execution_time_parallel = end_time_parallel - start_time_parallel\n",
        "print(f\"time taken for parallel: {execution_time_parallel} s\")\n"
      ],
      "metadata": {
        "id": "i91ocgS38Ykk",
        "outputId": "b9d242bd-2f60-4481-d589-dd3be35b7340",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time taken for parallel: 65.09306764602661 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features = normalize_features(features)"
      ],
      "metadata": {
        "id": "A9s0uW1c9VhB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Convert labels to numeric form\n",
        "label_mapping = {label: idx for idx, label in enumerate(set(labels))}\n",
        "numeric_labels = [label_mapping[label] for label in labels]\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VUjMuES6AXU4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets randomly\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, numeric_labels, test_size=0.2)\n",
        "\n",
        "# Ensure balanced data split\n",
        "print(f\"Number of samples in training set: {len(X_train)}\")\n",
        "print(f\"Number of samples in testing set: {len(X_test)}\")"
      ],
      "metadata": {
        "id": "J1vJBl-6CfKS",
        "outputId": "a5549592-8f48-4724-d9bd-5d818197c4dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples in training set: 6000\n",
            "Number of samples in testing set: 1501\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Define the SGDClassifier with logistic regression loss function\n",
        "model = SGDClassifier(loss='log_loss', max_iter=1000, learning_rate='constant', eta0=0.01)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict labels for the test set\n",
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "id": "D53juLb6Dcpq"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = model.score(X_test, y_test)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# Calculate confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Calculate classification report (F1 score, recall, precision)\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\")\n",
        "print(report)\n"
      ],
      "metadata": {
        "id": "sH5OzTCcDHTJ",
        "outputId": "4a20322b-36bd-4510-b6c2-2a3bb86223ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9653564290473018\n",
            "Confusion Matrix:\n",
            "[[267   7   3   1   4]\n",
            " [ 10 301   0   0   3]\n",
            " [  2   1 310   4   4]\n",
            " [  0   0   5 290   0]\n",
            " [  5   2   1   0 281]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.95      0.94       282\n",
            "           1       0.97      0.96      0.96       314\n",
            "           2       0.97      0.97      0.97       321\n",
            "           3       0.98      0.98      0.98       295\n",
            "           4       0.96      0.97      0.97       289\n",
            "\n",
            "    accuracy                           0.97      1501\n",
            "   macro avg       0.97      0.97      0.97      1501\n",
            "weighted avg       0.97      0.97      0.97      1501\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Logistic Regression model with gradient descent approach\n",
        "model = LogisticRegression(max_iter=1000, solver='sag', )\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict labels for the test set\n",
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "id": "Caj-xIHsJhoU"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = model.score(X_test, y_test)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# Calculate confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Calculate classification report (F1 score, recall, precision)\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\")\n",
        "print(report)\n"
      ],
      "metadata": {
        "id": "NaJWxQSPJw-d",
        "outputId": "f2e9a5e7-a4d6-4285-b468-bc0be8ec72ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9766822118587608\n",
            "Confusion Matrix:\n",
            "[[270   9   0   0   3]\n",
            " [  9 300   1   0   4]\n",
            " [  1   0 319   1   0]\n",
            " [  0   0   0 295   0]\n",
            " [  4   2   1   0 282]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.96      0.95       282\n",
            "           1       0.96      0.96      0.96       314\n",
            "           2       0.99      0.99      0.99       321\n",
            "           3       1.00      1.00      1.00       295\n",
            "           4       0.98      0.98      0.98       289\n",
            "\n",
            "    accuracy                           0.98      1501\n",
            "   macro avg       0.98      0.98      0.98      1501\n",
            "weighted avg       0.98      0.98      0.98      1501\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BRQYXRaRKBMv"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}